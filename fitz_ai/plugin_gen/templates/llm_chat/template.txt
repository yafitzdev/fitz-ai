# TEMPLATE: LLM Chat Plugin
#
# Chat plugins connect to LLM providers for text generation.
#
# REQUIRED FIELDS:
#   - plugin_name: Unique identifier
#   - plugin_type: Must be "chat"
#   - provider.base_url: API base URL
#   - auth.env_vars: Environment variable(s) for API key
#   - endpoint.path: API endpoint path
#   - defaults.models: Model names by tier (smart/fast/balanced)
#   - request.messages_transform: How to format messages
#   - response.content_path: JSONPath to extract response text

plugin_name: "CHANGE_ME"
plugin_type: "chat"
version: "1.0"

provider:
  name: "CHANGE_ME"
  base_url: "https://api.example.com/v1"  # API base URL

auth:
  type: "bearer"  # "bearer" | "header" | "none"
  header_name: "Authorization"
  header_format: "Bearer {key}"
  env_vars:
    - "EXAMPLE_API_KEY"  # Environment variable for API key

endpoint:
  path: "/chat/completions"  # API endpoint path
  method: "POST"
  timeout: 120

defaults:
  models:
    smart: "model-large"     # Best quality model
    fast: "model-small"      # Fastest model
    balanced: "model-medium" # Cost-effective model
  temperature: 0.2

request:
  # Message format: "openai_chat" | "anthropic_chat" | "cohere_chat"
  messages_transform: "openai_chat"
  param_map:
    model: "model"
    temperature: "temperature"
    max_tokens: "max_tokens"

response:
  # JSONPath to extract response text from API response
  content_path: "choices[0].message.content"
