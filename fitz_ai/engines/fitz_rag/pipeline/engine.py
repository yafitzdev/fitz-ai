# fitz_ai/engines/fitz_rag/pipeline/engine.py
"""
RAGPipeline - Core orchestration for retrieval-augmented generation.

Flow: retrieval → constraints → answer_mode → context-processing → rgs → llm → answer
"""

from __future__ import annotations

from typing import Sequence

from fitz_ai.core.answer_mode import AnswerMode
from fitz_ai.core.answer_mode_resolver import resolve_answer_mode
from fitz_ai.core.guardrails import (
    ConstraintPlugin,
    ConstraintResult,
    SemanticMatcher,
    create_default_constraints,
)
from fitz_ai.engines.fitz_rag.config import FitzRagConfig, load_config
from fitz_ai.engines.fitz_rag.exceptions import (
    LLMError,
    PipelineError,
    RGSGenerationError,
)
from fitz_ai.engines.fitz_rag.generation.answer_mode.instructions import (
    get_mode_instruction,
)
from fitz_ai.engines.fitz_rag.generation.retrieval_guided.synthesis import (
    RGS,
    RGSAnswer,
)
from fitz_ai.engines.fitz_rag.generation.retrieval_guided.synthesis import (
    RGSConfig as RGSRuntimeConfig,
)
from fitz_ai.engines.fitz_rag.pipeline.pipeline import ContextPipeline
from fitz_ai.engines.fitz_rag.retrieval.multihop import (
    BridgeExtractor,
    EvidenceEvaluator,
    HopController,
)
from fitz_ai.engines.fitz_rag.retrieval.registry import get_retrieval_plugin
from fitz_ai.engines.fitz_rag.routing import QueryIntent, QueryRouter
from fitz_ai.ingestion.entity_graph import EntityGraphStore
from fitz_ai.ingestion.vocabulary import KeywordMatcher, create_matcher_from_store
from fitz_ai.llm.registry import get_llm_plugin
from fitz_ai.logging.logger import get_logger
from fitz_ai.logging.tags import PIPELINE, VECTOR_DB
from fitz_ai.tabular.store import get_table_store
from fitz_ai.vector_db.registry import get_vector_db_plugin

logger = get_logger(__name__)


# =============================================================================
# RAGPipeline
# =============================================================================


class RAGPipeline:
    """
    RAG Pipeline orchestrator.

    Flow: retrieval → constraints → context-processing → rgs → llm → answer
    """

    def __init__(
        self,
        retrieval,  # RetrievalPipelineFromYaml
        chat,
        rgs: RGS,
        context: ContextPipeline | None = None,
        constraints: Sequence[ConstraintPlugin] | None = None,
        semantic_matcher: SemanticMatcher | None = None,
        query_router: QueryRouter | None = None,
        keyword_matcher: KeywordMatcher | None = None,
        hop_controller: HopController | None = None,
    ):
        self.retrieval = retrieval
        self.chat = chat
        self.rgs = rgs
        self.context = context or ContextPipeline()
        self.query_router = query_router
        self.keyword_matcher = keyword_matcher
        self.hop_controller = hop_controller

        # Default constraints: ConflictAware + InsufficientEvidence + CausalAttribution
        # Uses semantic embedding similarity for language-agnostic detection.
        # Users can override by passing constraints=[] to disable
        if constraints is None:
            if semantic_matcher is None:
                # No constraints if no semantic matcher provided
                # (caller should use from_config which provides embedder)
                self.constraints: list[ConstraintPlugin] = []
                logger.warning(
                    f"{PIPELINE} No semantic_matcher provided, constraints disabled. "
                    "Use RAGPipeline.from_config() for full constraint support."
                )
            else:
                self.constraints = create_default_constraints(semantic_matcher)
        else:
            self.constraints = list(constraints)

        routing_status = "enabled" if query_router and query_router.enabled else "disabled"
        keyword_status = "enabled" if keyword_matcher else "disabled"
        multihop_status = f"enabled (max {hop_controller.max_hops})" if hop_controller else "disabled"
        logger.info(
            f"{PIPELINE} RAGPipeline initialized with {retrieval.plugin_name} retrieval, "
            f"{len(self.constraints)} constraint(s), routing={routing_status}, "
            f"keywords={keyword_status}, multihop={multihop_status}"
        )

    def run(self, query: str) -> RGSAnswer:
        """Execute the RAG pipeline for a query."""
        logger.info(f"{PIPELINE} Running pipeline for query='{query[:50]}...'")

        # Step 0: Route query to appropriate retrieval target
        filter_override = None
        if self.query_router:
            intent = self.query_router.classify(query)
            if intent == QueryIntent.GLOBAL:
                filter_override = self.query_router.get_l2_filter()
                logger.info(f"{PIPELINE} Query routed to L2 corpus summaries (global intent)")

        # Step 1: Retrieve relevant chunks (using multi-hop or single retrieval)
        try:
            if self.hop_controller:
                raw_chunks, hop_metadata = self.hop_controller.retrieve(
                    query, filter_override=filter_override
                )
                logger.info(
                    f"{PIPELINE} Multi-hop retrieval: {hop_metadata.total_hops} hop(s), "
                    f"{len(raw_chunks)} total chunks"
                )
            else:
                raw_chunks = self.retrieval.retrieve(query, filter_override=filter_override)
        except Exception as exc:
            logger.error(f"{PIPELINE} Retrieval failed: {exc}")
            raise PipelineError("Retrieval failed") from exc

        # Step 1.5: Apply keyword filtering (if vocabulary exists)
        if self.keyword_matcher:
            filtered_chunks = self.keyword_matcher.filter_chunks(query, raw_chunks)
            if len(filtered_chunks) < len(raw_chunks):
                logger.info(
                    f"{PIPELINE} Keyword filter: {len(raw_chunks)} → {len(filtered_chunks)} chunks"
                )
            raw_chunks = filtered_chunks

        # Step 2: Apply constraints
        constraint_results = self._apply_all_constraints(query, raw_chunks)

        # Step 3: Resolve answer mode from constraint signals
        answer_mode = resolve_answer_mode(constraint_results)
        logger.info(f"{PIPELINE} Answer mode resolved: {answer_mode.value}")

        # Step 4: Process context (dedupe, group, merge, pack)
        try:
            chunks = self.context.process(raw_chunks)
        except Exception as exc:
            logger.error(f"{PIPELINE} Context processing failed: {exc}")
            raise PipelineError("Context processing failed") from exc

        # Step 5: Build RGS prompt with answer mode instruction
        try:
            prompt = self.rgs.build_prompt(query, chunks)
            prompt = self._apply_answer_mode_to_prompt(prompt, answer_mode)
        except Exception as exc:
            logger.error(f"{PIPELINE} Failed to build RGS prompt: {exc}")
            raise RGSGenerationError("Failed to build RGS prompt") from exc

        # Step 6: Generate answer via LLM
        messages = [
            {"role": "system", "content": prompt.system},
            {"role": "user", "content": prompt.user},
        ]

        try:
            raw = self.chat.chat(messages)
        except Exception as exc:
            logger.error(f"{PIPELINE} LLM chat failed: {exc}")
            raise LLMError("LLM chat operation failed") from exc

        # Step 7: Structure the answer with mode
        try:
            answer = self.rgs.build_answer(raw, chunks, mode=answer_mode)
            logger.info(f"{PIPELINE} Pipeline run completed (mode={answer_mode.value})")
            return answer
        except Exception as exc:
            logger.error(f"{PIPELINE} Failed to structure RGS answer: {exc}")
            raise RGSGenerationError("Failed to build RGS answer") from exc

    def _apply_all_constraints(
        self,
        query: str,
        chunks,
    ) -> list[ConstraintResult]:
        """Apply all constraints and return individual results."""
        results: list[ConstraintResult] = []

        for constraint in self.constraints:
            try:
                result = constraint.apply(query, chunks)
                results.append(result)
            except Exception as e:
                logger.warning(f"{PIPELINE} Constraint '{constraint.name}' raised exception: {e}")
                # Fail-safe: continue without this constraint's result
                continue

        return results

    def _apply_answer_mode_to_prompt(self, prompt, answer_mode: AnswerMode):
        """Prepend answer mode instruction to system prompt."""
        from fitz_ai.engines.fitz_rag.generation.retrieval_guided.synthesis import (
            RGSPrompt,
        )

        instruction = get_mode_instruction(answer_mode)
        modified_system = f"{instruction}\n\n{prompt.system}"

        return RGSPrompt(
            system=modified_system,
            user=prompt.user,
        )

    @classmethod
    def from_config(
        cls,
        cfg: FitzRagConfig,
        constraints: Sequence[ConstraintPlugin] | None = None,
        enable_keywords: bool = True,
    ) -> "RAGPipeline":
        """
        Create a RAGPipeline from configuration.

        Args:
            cfg: RAG configuration
            constraints: Optional list of constraints to override defaults
            enable_keywords: Whether to load keyword matcher from vocabulary store
        """
        logger.info(f"{PIPELINE} Constructing RAGPipeline from config")

        # Vector DB
        vector_client = get_vector_db_plugin(cfg.vector_db.plugin_name, **cfg.vector_db.kwargs)
        logger.info(f"{VECTOR_DB} Using vector DB plugin='{cfg.vector_db.plugin_name}'")

        # Chat LLM - use "smart" tier for user-facing query responses
        chat_plugin = get_llm_plugin(
            plugin_type="chat",
            plugin_name=cfg.chat.plugin_name,
            tier="smart",
            **cfg.chat.kwargs,
        )
        model_name = getattr(chat_plugin, "params", {}).get("model", "unknown")
        logger.info(
            f"{PIPELINE} Using chat plugin='{cfg.chat.plugin_name}' model='{model_name}' (smart tier)"
        )

        # Embedding
        embedder = get_llm_plugin(
            plugin_type="embedding",
            plugin_name=cfg.embedding.plugin_name,
            **cfg.embedding.kwargs,
        )
        logger.info(f"{PIPELINE} Using embedding plugin='{cfg.embedding.plugin_name}'")

        # Rerank (optional)
        reranker = None
        if cfg.rerank.enabled and cfg.rerank.plugin_name:
            reranker = get_llm_plugin(
                plugin_type="rerank",
                plugin_name=cfg.rerank.plugin_name,
                **cfg.rerank.kwargs,
            )
            logger.info(f"{PIPELINE} Using rerank plugin='{cfg.rerank.plugin_name}'")

        # Fast chat for multi-query expansion (uses same plugin, fast tier)
        fast_chat = get_llm_plugin(
            plugin_type="chat",
            plugin_name=cfg.chat.plugin_name,
            tier="fast",
            **cfg.chat.kwargs,
        )
        fast_model = getattr(fast_chat, "params", {}).get("model", "unknown")
        logger.info(
            f"{PIPELINE} Using chat plugin='{cfg.chat.plugin_name}' model='{fast_model}' "
            "(fast tier for multi-query expansion)"
        )

        # Keyword matcher (auto-loaded from collection's vocabulary if exists)
        # Must be created before retrieval pipeline so multi-query can use it
        keyword_matcher = None
        if enable_keywords:
            keyword_matcher = create_matcher_from_store(collection=cfg.retrieval.collection)
        if keyword_matcher:
            logger.info(
                f"{PIPELINE} Loaded vocabulary [{cfg.retrieval.collection}] "
                f"with {len(keyword_matcher.keywords)} keywords"
            )

        # Table store for CSV file queries
        # Uses GenericTableStore for remote vector DBs, SqliteTableStore for local
        table_store = get_table_store(
            collection=cfg.retrieval.collection,
            vector_db_plugin=cfg.vector_db.plugin_name,
            vector_plugin_instance=vector_client,
        )
        logger.info(f"{PIPELINE} Using table store for plugin='{cfg.vector_db.plugin_name}'")

        # Entity graph for related chunk discovery (auto-loaded from collection)
        entity_graph = None
        max_entity_expansion = cfg.entity_graph.max_expansion
        if cfg.entity_graph.enabled:
            entity_graph = EntityGraphStore(collection=cfg.retrieval.collection)
            graph_stats = entity_graph.stats()
            if graph_stats["entities"] > 0:
                logger.info(
                    f"{PIPELINE} Loaded entity graph [{cfg.retrieval.collection}] "
                    f"with {graph_stats['entities']} entities, {graph_stats['edges']} edges"
                )
            else:
                entity_graph = None  # No entities, don't use expansion
        else:
            logger.info(f"{PIPELINE} Entity graph disabled by config")

        # Retrieval (YAML-based plugin)
        retrieval = get_retrieval_plugin(
            plugin_name=cfg.retrieval.plugin_name,
            vector_client=vector_client,
            embedder=embedder,
            collection=cfg.retrieval.collection,
            reranker=reranker,
            chat=fast_chat,
            keyword_matcher=keyword_matcher,
            entity_graph=entity_graph,
            max_entity_expansion=max_entity_expansion,
            table_store=table_store,
            top_k=cfg.retrieval.top_k,
            fetch_artifacts=cfg.retrieval.fetch_artifacts,
        )
        logger.info(f"{PIPELINE} Using retrieval plugin='{cfg.retrieval.plugin_name}'")

        # RGS
        rgs_cfg = RGSRuntimeConfig(
            enable_citations=cfg.rgs.enable_citations,
            strict_grounding=cfg.rgs.strict_grounding,
            answer_style=cfg.rgs.answer_style,
            max_chunks=cfg.rgs.max_chunks,
            max_answer_chars=cfg.rgs.max_answer_chars,
            include_query_in_context=cfg.rgs.include_query_in_context,
            source_label_prefix=cfg.rgs.source_label_prefix,
        )
        rgs = RGS(config=rgs_cfg)

        # Create semantic matcher for constraints using the embedder
        semantic_matcher = SemanticMatcher(embedder=embedder.embed)

        # Query router (routes global queries to L2 summaries)
        query_router = QueryRouter(
            enabled=cfg.routing.enabled,
            embedder=embedder.embed,
            threshold=cfg.routing.threshold,
        )

        # Multi-hop controller (iterative evidence gathering)
        # Always on when fast_chat is available, but naturally stops after 1 hop
        # when evidence is sufficient
        hop_controller = None
        if fast_chat and cfg.multihop.max_hops > 1:
            evaluator = EvidenceEvaluator(chat=fast_chat)
            extractor = BridgeExtractor(chat=fast_chat)
            hop_controller = HopController(
                retrieval_pipeline=retrieval,
                evaluator=evaluator,
                extractor=extractor,
                max_hops=cfg.multihop.max_hops,
            )
            logger.info(
                f"{PIPELINE} Multi-hop retrieval enabled (max {cfg.multihop.max_hops} hops)"
            )

        logger.info(f"{PIPELINE} RAGPipeline successfully created")
        return cls(
            retrieval=retrieval,
            chat=chat_plugin,
            rgs=rgs,
            context=ContextPipeline(),
            constraints=constraints,
            semantic_matcher=semantic_matcher,
            query_router=query_router,
            keyword_matcher=keyword_matcher,
            hop_controller=hop_controller,
        )

    @classmethod
    def from_dict(cls, config_dict: dict) -> "RAGPipeline":
        """Create a RAGPipeline from a configuration dictionary."""
        cfg = FitzRagConfig.from_dict(config_dict)
        return cls.from_config(cfg)

    @classmethod
    def from_yaml(cls, config_path: str) -> "RAGPipeline":
        """Create a RAGPipeline from a YAML configuration file."""
        cfg = load_config(config_path)
        return cls.from_config(cfg)


def create_pipeline_from_yaml(path: str | None = None) -> RAGPipeline:
    """Create a RAGPipeline from a YAML config file."""
    cfg = load_config(path)
    return RAGPipeline.from_config(cfg)
