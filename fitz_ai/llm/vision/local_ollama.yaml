# fitz_ai/llm/vision/local_ollama.yaml
#
# Ollama Vision Plugin - Local Vision LLM
# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md
# No API key required - runs locally
# Requires vision-capable model: llama3.2-vision, llava, bakllava

# =============================================================================
# IDENTITY
# =============================================================================
plugin_name: "ollama"
plugin_type: "vision"
version: "1.0"

# =============================================================================
# PROVIDER
# =============================================================================
provider:
  name: "ollama"
  base_url: "http://localhost:11434"

# =============================================================================
# AUTHENTICATION
# =============================================================================
auth:
  type: "none"
  header_name: ""
  header_format: ""
  env_vars: []

# =============================================================================
# REQUIRED ENVIRONMENT VARIABLES
# =============================================================================
required_env: []

# =============================================================================
# HEALTH CHECK
# =============================================================================
health_check:
  path: "/api/tags"
  method: "GET"
  timeout: 2

# =============================================================================
# ENDPOINT
# =============================================================================
endpoint:
  path: "/api/chat"
  method: "POST"
  timeout: 300  # 5 minutes - VLM may take longer on first call while loading model

# =============================================================================
# DEFAULTS
# =============================================================================
# Vision models available in Ollama:
#   - llama3.2-vision (recommended, ~8GB VRAM)
#   - llava (older, ~8GB VRAM)
#   - bakllava (faster, ~4GB VRAM)
defaults:
  models:
    smart: "llama3.2-vision"
    fast: "llava"
    balanced: "llama3.2-vision"
  temperature: 0.2
  max_tokens: null
  # Default prompt for figure description
  default_prompt: "Describe this figure/chart/diagram in detail. Include any data values, labels, axes, trends, and key insights visible in the image."

# =============================================================================
# REQUEST CONFIGURATION
# =============================================================================
request:
  messages_transform: "ollama_vision"
  static_fields:
    stream: false
  param_map:
    model: "model"
    temperature: "options.temperature"
    max_tokens: "max_tokens"

# =============================================================================
# RESPONSE CONFIGURATION
# =============================================================================
response:
  content_path: "message.content"
  is_array: false
  array_index: 0
  metadata_paths:
    done: "done"
    total_duration: "total_duration"
    eval_count: "eval_count"
