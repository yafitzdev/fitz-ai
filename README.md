# fitz-ai

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyPI version](https://badge.fury.io/py/fitz-ai.svg)](https://pypi.org/project/fitz-ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.4.5-green.svg)](CHANGELOG.md)
[![Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen)](https://github.com/yafitzdev/fitz-ai)

---

**Honest RAG in 5 minutes. No infrastructure. No boilerplate.**

```bash
pip install fitz-ai

fitz quickstart ./docs "What is our refund policy?"
```

That's it. Your documents are now searchable with AI.


![fitz-ai quickstart demo](https://raw.githubusercontent.com/yafitzdev/fitz-ai/main/docs/assets/quickstart_demo.gif)

<br>

<details>

<summary><strong>Python SDK</strong></summary>

<br>

```python
import fitz_ai

fitz_ai.ingest("./docs")
answer = fitz_ai.query("What is our refund policy?")
```

</details>

<br>

<details>

<summary><strong>REST API</strong></summary>

<br>

```bash
pip install fitz-ai[api]

fitz serve  # http://localhost:8000/docs for interactive API
```

</details>

---

### About ğŸ§‘â€ğŸŒ¾

  Solo project by Yan Fitzner ([LinkedIn](https://www.linkedin.com/in/yan-fitzner/), [GitHub](https://github.com/yafitzdev)).

  - ~55k lines of Python
  - 700+ tests, 100% coverage
  - Zero LangChain/LlamaIndex dependencies â€” built from scratch

![fitz-ai honest_rag](https://raw.githubusercontent.com/yafitzdev/fitz-ai/main/docs/assets/honest_rag.jpg)

---

<details>

<summary><strong>ğŸ“¦ What is RAG?</strong></summary>

<br>

RAG is how ChatGPT's "file search," Notion AI, and enterprise knowledge tools actually work under the hood.
Instead of sending all your documents to an AI, RAG:

1. [X] **Indexes your documents once** â€” Splits them into chunks, converts to vectors, stores in a database
2. [X] **Retrieves only what's relevant** â€” When you ask a question, finds the 5-10 most relevant chunks
3. [X] **Sends just those chunks to the LLM** â€” The AI answers based on focused, relevant context

Traditional approach:
```
  [All 10,000 documents] â†’ LLM â†’ Answer
  âŒ Impossible (too large)
  âŒ Expensive (if possible)
  âŒ Unfocused
```
RAG approach:
```
  Question â†’ [Search index] â†’ [5 relevant chunks] â†’ LLM â†’ Answer
  âœ… Works at any scale
  âœ… Costs pennies per query
  âœ… Focused context = better answers
```

</details>

---

<details>

<summary><strong>ğŸ“¦ Why Can't I Just Send My Documents to ChatGPT directly?</strong></summary>

<br>

You canâ€”but you'll hit walls fast.

**Context window limits ğŸš¨** 
> GPT-4 accepts ~128k tokens. That's roughly 300 pages. Your company wiki, codebase, or document archive is likely 10x-100x larger. You physically cannot paste it all.

**Cost explosion ğŸ’¥**
> Even if you could fit everything, you'd pay for every token on every query. Sending 100k tokens costs ~\$1-3 per question. Ask 50 questions a day? That's $50-150 dailyâ€”for one user.

**No selective retrieval âŒ**
> When you paste documents, the model reads everything equally. It can't focus on what's relevant. Ask about refund policies and it's also processing your hiring guidelines, engineering specs, and meeting notesâ€”wasting context and degrading answers.

**No persistence ğŸ’¢**
> Every conversation starts fresh. You re-upload, re-paste, re-explain. There's no knowledge base that accumulates and improves.

</details>

---

### Why Fitz?

**Super fast setup ğŸ†**
> Point at a folder. Ask a question. Get an answer with sources. Everything else is handled by Fitz.

**Honest answers âœ…**
> Most RAG tools confidently answer even when the answer isn't in your documents. Ask "What was our Q4 revenue?" when your docs only cover Q1-Q3, and typical RAG hallucinates a number. Fitz says: *"I cannot find Q4 revenue figures in the provided documents."*

**Swap engines, keep everything else âš™ï¸**
> RAG is evolving fastâ€”GraphRAG, HyDE, ColBERT, whatever's next. Fitz lets you switch engines in one line. Your ingested data stays. Your queries stay. No migration, no re-ingestion, no new API to learn. Frameworks lock you in; Fitz lets you move.

**Analytical queries that actually work ğŸ“Š**
> Standard RAG fails on questions like "What are the trends?"â€”it retrieves random chunks instead of insights. Fitz's hierarchical RAG generates multi-level summaries during ingestion. Ask for trends, get aggregated analysis. Ask for specifics, get detail chunks. No special syntax required.

**Other Features at a Glance ğŸƒ**
>
>1. [x] **Local execution possible.** FAISS and Ollama support, no API keys required to start.
>2. [x] **Plugin-based architecture.** Swap LLMs, vector databases, rerankers, and retrieval pipelines via YAML config.
>3. [x] **Multiple engines.** Supports FitzRAG, GraphRAG and CLaRa out of the boxâ€”swap engines in one line.
>4. [X] **Incremental ingestion.** Only reprocesses changed files, even with new chunking settings.
>5. [x] **Full provenance.** Every answer traces back to the exact chunk and document.
>6. [x] **Data privacy**: No telemetry, no cloud, no external calls except to the LLM provider you configure.

####

Any questions left? Try fitz on itself:

```bash
fitz quickstart ./fitz_ai "How does the chunking pipeline work?"
```

The codebase speaks for itself.

---

<details>

<summary><strong>ğŸ“¦ Fitz vs LangChain vs LlamaIndex</strong></summary>

<br>

#### Fitz opts for a deliberately narrower approach.
>
>LangChain and LlamaIndex are powerful **LLM application frameworks** designed to help developers build complex, end-to-end AI systems. 
>Fitz provides a **minimal, replaceable RAG engine** with strong epistemic guarantees â€” without locking users into a framework, ecosystem, or long-term architectural commitment.
>
>Fitz is not a competitor in scope.  
>It is an infrastructure primitive.

<br>

#### Core philosophical differences âš–ï¸
>
>| Dimension | Fitz | LangChain | LlamaIndex |
>|--------|------|-----------|------------|
>| Primary role | **RAG engine** | LLM application framework | LLM data framework |
>| User commitment | **No framework lock-in** | High | High |
>| Engine coupling | **Swappable in one line** | Deep | Deep |
>| Design goal | Correctness & honesty | Flexibility | Data integration |
>| Long-term risk | Low | Migration-heavy | Migration-heavy |

<br>

#### Epistemic behavior (truth over fluency) ğŸ¯
>
>| Aspect | Fitz | LangChain / LlamaIndex |
>|-----|------|------------------------|
>| â€œI donâ€™t knowâ€ | **First-class behavior** | Not guaranteed |
>| Hallucination handling | Designed-in | Usually prompt-level |
>| Confidence signaling | Explicit | Implicit |
>
>Fitz treats uncertainty as a **feature**, not a failure.  
>If the system cannot support an answer with retrieved evidence, it says so.

<br>

#### Transparency & provenance ğŸ”
>
>| Capability | Fitz | LangChain / LlamaIndex |
>|---------|------|------------------------|
>| Source attribution | **Mandatory** | Optional |
>| Retrieval trace | **Explicit & structured** | Often opaque |
>| Debuggability | Built-in | Tool-dependent |
>
>Every answer in Fitz is fully auditable down to the retrieval step.

<br>

#### Scope & complexity ğŸª
>
>| Aspect | Fitz | LangChain / LlamaIndex |
>|-----|------|------------------------|
>| Chains / agents | â | âœ” |
>| Prompt graphs | â | âœ” |
>| UI abstractions | â | Often |
>| Cognitive overhead | **Very low** | High |
>
>Fitz intentionally does less â€” so it can be trusted more.

<br>

#### Use Fitz if you want:
>
>- A replaceable RAG engine, not a framework marriage
>- Strong epistemic guarantees (â€œI donâ€™t knowâ€ is valid output)
>- Full provenance for every answer
>- A transparent, extensible plugin architecture
>- A future-proof ingestion pipeline that survives engine changes

</details>

---

<details>

<summary><strong>ğŸ“¦ Fitz RAG vs GraphRAG</strong></summary>

<br>

> **"RAG is dead"** posts flood the AI scene. The argument: traditional RAG can't handle relationships or trends. GraphRAG is the new hotness.
>
> **Traditional RAG has two problemsâ€”it can't see the forest for the trees, and it lies about what it sees.**
>
> Fitz RAG solves both: **hierarchical summaries** for the big picture, **epistemic guardrails** for honesty. And now it also extracts entities and relationshipsâ€”without the graph construction overhead.

<br>

#### The real problem with RAG isn't retrievalâ€”it's confidence

>Most RAG failures aren't "couldn't find the relationship." They're:
>- Hallucinated answers presented confidently
>- Conflicting sources silently collapsed into one answer
>- Causality invented from correlation
>
>**GraphRAG doesn't solve any of these.** It just finds relationships better.
>
>Fitz RAG solves them with **epistemic guardrails**:
>
>| Problem | GraphRAG | Fitz RAG |
>|---------|----------|----------|
>| Sources disagree | Picks one silently | **DISPUTED mode** â€” surfaces the conflict |
>| No evidence for claim | Answers anyway | **ABSTAIN mode** â€” refuses to guess |
>| Correlation â‰  causation | Invents "why" | **Blocks causal hallucination** |
>| Uncertain answer | Sounds confident | **QUALIFIED mode** â€” notes limitations |

<br>

#### What Fitz RAG now shares with GraphRAG

>Fitz RAG has closed the gap on key GraphRAG featuresâ€”without the complexity:
>
>| Capability | GraphRAG | Fitz RAG |
>|------------|----------|----------|
>| **Entity extraction** | LLM extracts entities | LLM extracts entities (classes, functions, APIs, people, orgs) |
>| **Entity relationships** | Full knowledge graph | Co-occurrence links (entities in same chunk are linked) |
>| **Semantic clustering** | Leiden community detection | K-means clustering by embedding similarity |
>| **Trend analysis** | Community summaries | Hierarchical summaries (L0â†’L1â†’L2) |
>
>The difference: Fitz extracts entities and links them **without building a graph**. Co-occurrence linking captures 80% of useful relationships at 10% of the complexity.

<br>

#### When GraphRAG still wins

>GraphRAG excels at **multi-hop relationship traversal**:
>
>| Use Case | Why GraphRAG |
>|----------|--------------|
>| "Who founded the company that acquired Z?" | Multi-hop graph traversal |
>| Complex relationship chains | Explicit edge following |
>| Visual knowledge exploration | Graph visualization |
>
>If you need to traverse 3+ hop relationships or visualize entity networks, GraphRAG is the right tool.

<br>

#### When Fitz RAG wins

>Fitz RAG excels at **trusted answers, entities, and analytical queries**:
>
>| Use Case | Why Fitz RAG |
>|----------|--------------|
>| Q&A where trust matters | Epistemic guardrails |
>| "What entities are in this doc?" | **Entity extraction** with type filtering |
>| "What concepts co-occur?" | **Entity linking** (co-occurrence) |
>| "What are the trends?" | **Hierarchical summaries** (L0â†’L1â†’L2) |
>| "Summarize this corpus" | **Corpus-level summaries** auto-generated |
>| Conflicting sources | Conflict detection |
>| Compliance/legal queries | Admits uncertainty |
>| Fast, cheap retrieval | No graph construction |
>| Incremental updates | Just add new chunks |
>
>**Fitz RAG extracts entities, links them, clusters them, and summarizes them**â€”all during ingestion:
>- **Entities**: Classes, functions, APIs, people, organizations, concepts
>- **Links**: Co-occurrence relationships stored in chunk metadata
>- **Clusters**: Semantic grouping via K-means on embeddings
>- **Summaries**: L0 chunks â†’ L1 group summaries â†’ L2 corpus summary

<br>

#### The cost difference

>| Aspect | GraphRAG | Fitz RAG |
>|--------|----------|----------|
>| Ingest cost | **High** â€” LLM extracts entities + builds graph | **Medium** â€” LLM extracts entities (optional) |
>| Ingest speed | Slow â€” graph construction | Fast â€” no graph building |
>| Query latency | Higher â€” graph traversal | Lower â€” vector search |
>| Error propagation | Bad extraction = bad graph | Entities are metadata, not structure |
>| Schema dependency | Must define entity types | Flexible type list |
>| Incremental updates | Rebuild graph sections | Just add chunks + entities |

<br>

#### The bottom line

>| Capability | GraphRAG | Fitz RAG |
>|------------|----------|----------|
>| Entity extraction | âœ… LLM-based | âœ… LLM-based |
>| Entity relationships | Full knowledge graph | Co-occurrence links |
>| Semantic clustering | Leiden algorithm | K-means on embeddings |
>| Trend analysis | Community summaries | **Hierarchical summaries** |
>| Corpus overview | Global search | **L2 corpus summary** |
>| Epistemic safety | âŒ None | âœ… **Guardrails built-in** |
>
>**GraphRAG wins on multi-hop traversal. Fitz RAG wins on trusted answers + entities + trends.**
>
>For most enterprise use casesâ€”support, compliance, internal knowledge, trend analysisâ€”Fitz RAG now delivers 90% of GraphRAG's capabilities at a fraction of the cost, plus epistemic guarantees GraphRAG simply doesn't have.
>
>Need the full graph? Fitz gives you both engines. Same data. Same API. Choose per query.

</details>

---
<details>

<summary><strong>ğŸ“¦ Features</strong></summary>

<br>

#### Hierarchical RAG ğŸ“Š

>Standard RAG struggles with analytical queries like "What are the trends?" because it retrieves random chunks instead of aggregated insights. Hierarchical RAG solves this.
>
>**The problem â˜”ï¸**
>```
>Q: "What are the trends in my comments?"
>Standard RAG: Returns random individual comments (not useful)
>```
>
>**The solution â˜€ï¸**
>
>For documents, Fitz auto-enables hierarchy when an LLM is available. It groups by file and generates multi-level summaries:
>- **Level 0**: Original chunks (unchanged)
>- **Level 1**: Group summaries (one per source file)
>- **Level 2**: Corpus summary (aggregates all groups)
>
>**Example: YouTube comment analysis**
>```
>Ingested: 500 comments across 10 videos
>
>Level 0: "This tutorial helped me understand async/await finally!"
>Level 1: "Tutorial Video #3: 47 comments, mostly positive. Users praise
>         clarity of examples. Common request: more on error handling."
>Level 2: "Across 10 videos (500 comments): 78% positive sentiment.
>         Top themes: code clarity, pacing, example quality.
>         Recurring requests: longer videos, more advanced topics."
>```
>
>Now analytical queries retrieve summaries, while specific queries still retrieve details:
>```
>Q: "What are the overall trends in my comments?"
>â†’ Returns Level 2 corpus summary + Level 1 video summaries
>```
>```
>Q: "What did people say about my async tutorial?"
>â†’ Returns Level 0 individual comments from that video
>```
>
>No special query syntax. No retrieval config changes. Summaries match analytical queries naturally via vector similarity.

<br>

#### Actually admits when it doesn't know ğŸ“š

> When documents don't contain the answer, fitz says so:
>
> ```
> Q: "What was our Q4 revenue?"
> A: "I cannot find Q4 revenue figures in the provided documents.
>     The available financial data covers Q1-Q3 only."
>
>    Mode: ABSTAIN
>```
>
>Three constraint plugins run automatically:
>1. [X] **ğŸ“• ConflictAwareConstraint**: Detects contradictions across sources
>2. [X] **ğŸ“— InsufficientEvidenceConstraint**: Blocks answers without evidence
>3. [X] **ğŸ“˜ CausalAttributionConstraint**: Prevents hallucinated cause-effect claims

<br>

#### Swappable RAG Engines ğŸ”„

>Your data stays. Your queries stay. Only the engine changes.
>
>```
>        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>        â”‚           Your Query                â”‚
>        â”‚   "What are the payment terms?"     â”‚
>        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
>                           â”‚
>                           â–¼
>        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>        â”‚       engine="..."                  â”‚
>        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
>        â”‚  â”‚ fitz    â”‚ â”‚ clara â”‚ â”‚ graph   â”‚  â”‚
>        â”‚  â”‚  _rag   â”‚ â”‚       â”‚ â”‚  _rag   â”‚  â”‚
>        â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
>        â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
>        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
>                           â”‚
>                           â–¼
>        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>        â”‚       Your Ingested Knowledge       â”‚
>        â”‚      (unchanged across engines)     â”‚
>        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
>```
>
>```python
>answer = run("What are the payment terms?", engine="fitz_rag")
>answer = run("What are the payment terms?", engine="clara")
>answer = run("What are the payment terms?", engine="graph_rag")  # future
>```
>
>No migration. No re-ingestion. No new API to learn.

<br>

#### Full Provenance ğŸ—‚ï¸

>Every answer traces back to its source:
>
>```
>Answer: The refund policy allows returns within 30 days...
>
>Sources:
>  [1] policies/refund.md [chunk 3] (score: 0.92)
>  [2] faq/payments.md [chunk 1] (score: 0.87)
>```

<br>

#### Incremental Ingestion âš¡

>Fitz tracks file hashes and only re-ingests what changed:
>
>```
>$ fitz ingest ./src
>
>Scanning... 847 files
>  â†’ 12 new files
>  â†’ 3 modified files
>  â†’ 832 unchanged (skipped)
>
>Ingesting 15 files...
>```
>
>Re-running ingestion on a large codebase takes seconds, not minutes. Changed your chunking config? Fitz detects that too and re-processes affected files.

<br>

#### Smart Chunking ğŸ§ 

>Format-aware chunking that preserves structure:
>
>| Format | Strategy |
>|--------|----------|
>| **Python** | AST-aware: keeps classes, functions, imports intact. Large classes split by method. |
>| **Markdown** | Header-aware: splits on `#` headers, preserves code blocks and lists. Extracts YAML frontmatter as metadata. |
>| **PDF** | Section-aware: detects numbered headings (1.1, 2.3.1), roman numerals, and keywords (Abstract, Conclusion). |
>
>No more retrieving half a function or a code block split mid-syntax.

</details>

---

<details>

<summary><strong>ğŸ“¦ Plugin Generator</strong></summary>

<br>

#### Generate plugins with AI ğŸ¤–

>Fitz can generate fully working plugins from natural language descriptions. Describe what you want, and fitz creates, validates, and saves the plugin automatically.
>
>```bash
>fitz plugin
>? Plugin type: chunker
>? Description: sentence-based chunker that splits on periods
>
>Generating...
>âœ“ Syntax valid
>âœ“ Schema valid
>âœ“ Plugin loads correctly
>âœ“ Functional test passed
>
>Created: ~/.fitz/plugins/chunking/sentence_chunker.py
>```
>
>The generated plugin is immediately usableâ€”no manual editing required.

<br>

#### Supported plugin types

>| Type | Format | Description |
>|------|--------|-------------|
>| `llm-chat` | YAML | Connect to a chat LLM provider |
>| `llm-embedding` | YAML | Connect to an embedding provider |
>| `llm-rerank` | YAML | Connect to a reranking provider |
>| `vector-db` | YAML | Connect to a vector database |
>| `retrieval` | YAML | Define a retrieval strategy |
>| `chunker` | Python | Custom document chunking logic |
>| `reader` | Python | Custom file format reader |
>| `constraint` | Python | Epistemic safety guardrail |

<br>

#### How it works

>1. **Prompt building**: Fitz loads existing plugin examples and schema definitions
>2. **Generation**: Your configured LLM generates the plugin code
>3. **Multi-level validation**: Syntax â†’ Schema â†’ Integration â†’ Functional tests
>4. **Auto-retry**: If validation fails, fitz feeds the error back and retries (up to 3 attempts)
>5. **Save**: Working plugins are saved to `~/.fitz/plugins/`
>
>Generated plugins are auto-discovered by fitz on next runâ€”no registration needed.

<br>

#### Example: Custom chunker

>```bash
>fitz plugin
>? Plugin type: chunker
>? Description: splits text by paragraphs, keeping code blocks intact
>
># Creates ~/.fitz/plugins/chunking/paragraph_chunker.py
>```
>
>```python
># Generated plugin is immediately usable
>fitz ingest ./docs --chunker paragraph_chunker
>```

</details>

---

<details>

<summary><strong>ğŸ“¦ Quick Start</strong></summary>

<br>

#### CLI
>
>```bash
>pip install fitz-ai
>
>fitz quickstart ./docs "Your question here"
>```
>
>Fitz auto-detects your LLM provider:
>1. **Ollama running?** â†’ Uses it automatically (fully local)
>2. **`COHERE_API_KEY` or `OPENAI_API_KEY` set?** â†’ Uses it automatically
>3. **First time?** â†’ Guides you through free Cohere signup (2 minutes)
>
>After first run, it's completely zero-friction.

<br>

#### Python SDK
>
>```python
>import fitz_ai
>
>fitz_ai.ingest("./docs")
>answer = fitz_ai.query("Your question here")
>
>print(answer.text)
>for source in answer.provenance:
>    print(f"  - {source.source_id}: {source.excerpt[:50]}...")
>```
>
>The SDK provides:
>- Module-level functions matching CLI (`ingest`, `query`)
>- Auto-config creation (no setup required)
>- Full provenance tracking
>- Same honest RAG as the CLI
>
>For advanced use (multiple collections), use the `fitz` class directly:
>```python
>from fitz_ai import fitz
>
>physics = fitz(collection="physics")
>physics.ingest("./physics_papers")
>answer = physics.query("Explain entanglement")
>```

<br>

#### Fully Local (Ollama)
>
>```bash
>pip install fitz-ai[local]
>
>ollama pull llama3.2
>ollama pull nomic-embed-text
>
>fitz quickstart ./docs "Your question here"
>```
>
>Fitz auto-detects Ollama when running. No API keys neededâ€”no data leaves your machine.

</details>

---

<details>

<summary><strong>ğŸ“¦ Real-World Usage</strong></summary>

<br>

Fitz is a foundation. It handles document ingestion and grounded retrievalâ€”you build whatever sits on top: chatbots, dashboards, alerts, or automation.

<br>

<strong>Chatbot Backend ğŸ¤–</strong>

> Connect fitz to Slack, Discord, Teams, or your own UI. One function call returns an answer with sourcesâ€”no hallucinations, full provenance. You handle the conversation flow; fitz handles the knowledge.
>
> *Example:* A SaaS company plugs fitz into their support bot. Tier-1 questions like "How do I reset my password?" get instant answers. Their support team focuses on edge cases while fitz deflects 60% of incoming tickets.

<br>

<strong>Internal Knowledge Base ğŸ“–</strong>

> Point fitz at your company's wiki, policies, and runbooks. Employees ask natural language questions instead of hunting through folders or pinging colleagues on Slack.
>
> *Example:* A 200-person startup ingests their Notion workspace and compliance docs. New hires find answers to "How do I request PTO?" on day oneâ€”no more waiting for someone in HR to respond.

<br>

<strong>Continuous Intelligence & Alerting (Watchdog) ğŸ¶</strong>

> Pair fitz with cron, Airflow, or Lambda. Ingest data on a schedule, run queries automatically, trigger alerts when conditions match. Fitz provides the retrieval primitive; you wire the automation.
>
> *Example:* A security team ingests SIEM logs nightly. Every morning, a scheduled job asks "Were there failed logins from unusual locations?" If fitz finds evidence, an alert fires to the on-call channel before anyone checks email.

<br>

<strong>Web Knowledge Base ğŸŒ</strong>

> Scrape the web with Scrapy, BeautifulSoup, or Playwright. Save to disk, ingest with fitz. The web becomes a queryable knowledge base.
>
> *Example:* A football analytics hobbyist scrapes Premier League match reports. After ingesting, they ask "How did Arsenal perform against top 6 teams?" or "What tactics did Liverpool use in away games?"â€”insights that would take hours to compile manually.

<br>

<strong>Codebase Search ğŸ</strong>

> Fitz includes built-in AST-aware chunking for code bases. Functions, classes, and modules become individual searchable units with docstrings and imports preserved. Ask questions in natural language; get answers pointing to specific code.
>
> *Example:* A team inherits a legacy Django monolithâ€”200k lines, sparse docs. They ingest the codebase and ask "Where is user authentication handled?" or "What API endpoints modify the billing table?" New developers onboard in days instead of weeks.

</details>

---

<details>

<summary><strong>ğŸ“¦ Architecture</strong></summary>

<br>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         fitz-ai                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Interfaces                                              â”‚
â”‚  CLI: quickstart | init | ingest | query | chat | serve       â”‚
â”‚  SDK: fitz_ai.fitz() â†’ ingest() â†’ ask()                       â”‚
â”‚  API: /query | /chat | /ingest | /collections | /health       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Engines                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  FitzRAG  â”‚  â”‚   CLaRa   â”‚  â”‚  GraphRAG  â”‚  (pluggable)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Plugin System (all YAML-defined)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Chat  â”‚ â”‚ Embedding â”‚ â”‚ Rerank â”‚ â”‚ VectorDB â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  openai, cohere, anthropic, ollama, azure...                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Retrieval Pipelines (YAML-composed)                          â”‚
â”‚  dense.yaml | dense_rerank.yaml | custom...                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Enrichment (opt-in)                                          â”‚
â”‚  entities | entity links | semantic clusters | hierarchical   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Constraints (epistemic safety)                               â”‚
â”‚  ConflictAware | InsufficientEvidence | CausalAttribution     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>

---

<details>

<summary><strong>ğŸ“¦ CLI Reference</strong></summary>

<br>

```bash
fitz quickstart [PATH] [QUESTION]    # Zero-config RAG (start here)
fitz init                            # Interactive setup wizard
fitz ingest                          # Interactive ingestion
fitz query                           # Single question with sources
fitz chat                            # Multi-turn conversation with your knowledge base
fitz collections                     # List and delete knowledge collections
fitz plugin                          # Generate plugins with AI
fitz serve                           # Start REST API server
fitz config                          # View/edit configuration
fitz doctor                          # System diagnostics
```

</details>

---

<details>

<summary><strong>ğŸ“¦ Python SDK Reference</strong></summary>

<br>

**Simple usage (module-level, matches CLI):**
```python
import fitz_ai

fitz_ai.ingest("./docs")
answer = fitz_ai.query("What is the refund policy?")
print(answer.text)
```

<br>

**Advanced usage (multiple collections):**
```python
from fitz_ai import fitz

# Create separate instances for different collections
physics = fitz(collection="physics")
physics.ingest("./physics_papers")

legal = fitz(collection="legal")
legal.ingest("./contracts")

# Query each collection
physics_answer = physics.query("Explain entanglement")
legal_answer = legal.query("What are the payment terms?")
```

<br>

**Working with answers:**
```python
answer = fitz_ai.query("What is the refund policy?")

print(answer.text)
print(answer.mode)  # CONFIDENT, QUALIFIED, DISPUTED, or ABSTAIN

for source in answer.provenance:
    print(f"Source: {source.source_id}")
    print(f"Excerpt: {source.excerpt}")
```

</details>

---

<details>

<summary><strong>ğŸ“¦ REST API Reference</strong></summary>

<br>

**Start the server:**
```bash
pip install fitz-ai[api]

fitz serve                    # localhost:8000
fitz serve -p 3000            # custom port
fitz serve --host 0.0.0.0     # all interfaces
```

**Interactive docs:** Visit `http://localhost:8000/docs` for Swagger UI.

<br>

**Endpoints:**

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/query` | Query knowledge base |
| POST | `/chat` | Multi-turn chat (stateless) |
| POST | `/ingest` | Ingest documents from path |
| GET | `/collections` | List all collections |
| GET | `/collections/{name}` | Get collection stats |
| DELETE | `/collections/{name}` | Delete a collection |
| GET | `/health` | Health check |

<br>

**Example requests:**

```bash
# Query
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the refund policy?", "collection": "default"}'

# Ingest
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{"source": "./docs", "collection": "mydata"}'

# Chat (stateless - client manages history)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What about returns?",
    "history": [
      {"role": "user", "content": "What is the refund policy?"},
      {"role": "assistant", "content": "The refund policy allows..."}
    ],
    "collection": "default"
  }'
```

</details>

---

<details>

<summary><strong>ğŸ“¦ Beyond RAG</strong></summary>

<br>

> **RAG is a method. Knowledge access is a strategy.**

Fitz is not a RAG framework. It's a knowledge platform that *currently* uses RAG as its primary engine.

```python
from fitz_ai import run

# Fitz RAG - fast, reliable vector search
answer = run("What are the payment terms?", engine="fitz_rag")

# CLaRa - compressed RAG, 16x smaller context
answer = run("What are the payment terms?", engine="clara")

# GraphRAG - knowledge graph with entity extraction and community summaries
answer = run("What are the payment terms?", engine="graphrag")
```

The engine is an implementation detail. Your ingested knowledge, your queries, your workflowâ€”all stay the same. When a better retrieval paradigm emerges, swap one line, not your entire codebase.

</details>

---

<details>

<summary><strong>ğŸ“¦ Philosophy</strong></summary>

<br>

**Principles:**
- **Explicit over clever**: No magic. Read the config, know what happens.
- **Answers over architecture**: Optimize for time-to-insight, not flexibility.
- **Honest over helpful**: Better to say "I don't know" than hallucinate.
- **Files over frameworks**: YAML plugins over class hierarchies.

</details>

---

### License

MIT

---

### Links

- [GitHub](https://github.com/yafitzdev/fitz-ai)
- [PyPI](https://pypi.org/project/fitz-ai/)
- [Changelog](CHANGELOG.md)
- [CLI Documentation](docs/CLI.md)
